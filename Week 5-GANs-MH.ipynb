{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(monet_dir, photo_dir):\n",
    "    print(\"Analyzing dataset structure and properties...\")\n",
    "\n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(monet_dir) or not os.path.exists(photo_dir):\n",
    "        print(\"Error: One or both directories do not exist.\")\n",
    "        return\n",
    "\n",
    "    # Count files\n",
    "    monet_files = os.listdir(monet_dir)\n",
    "    photo_files = os.listdir(photo_dir)\n",
    "\n",
    "    print(\"\\nDataset Structure:\")\n",
    "    print(f\"Number of Monet paintings: {len(monet_files)}\")\n",
    "    print(f\"Number of photographs: {len(photo_files)}\")\n",
    "\n",
    "    # Load and check first image from each set\n",
    "    try:\n",
    "        monet_img = Image.open(os.path.join(monet_dir, monet_files[0]))\n",
    "        photo_img = Image.open(os.path.join(photo_dir, photo_files[0]))\n",
    "\n",
    "        print(\"\\nImage Properties:\")\n",
    "        print(f\"Monet image dimensions: {monet_img.size}, Mode: {monet_img.mode}\")\n",
    "        print(f\"Photo image dimensions: {photo_img.size}, Mode: {photo_img.mode}\")\n",
    "\n",
    "        # Display sample images\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax1.imshow(monet_img)\n",
    "        ax1.set_title(\"Sample Monet Painting\")\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(photo_img)\n",
    "        ax2.set_title(\"Sample Photograph\")\n",
    "        ax2.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading images: {str(e)}\")\n",
    "\n",
    "# Define data directories\n",
    "monet_dir = 'data/monet_jpg'\n",
    "photo_dir = 'data/photo_jpg'\n",
    "\n",
    "# Run the analysis\n",
    "analyze_dataset(monet_dir, photo_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize random number generator for consistency\n",
    "random.seed(42)\n",
    "\n",
    "# Choose random image samples\n",
    "art_examples = random.sample(os.listdir(art_folder), 3)\n",
    "real_examples = random.sample(os.listdir(real_folder), 3)\n",
    "\n",
    "# Set up the display canvas\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Display real-world photographs\n",
    "for idx, image_file in enumerate(real_examples):\n",
    "    plt.subplot(2, 3, idx+4)\n",
    "    photo = Image.open(os.path.join(real_folder, image_file))\n",
    "    plt.imshow(photo)\n",
    "    plt.title('Real-world Scene')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display artistic paintings\n",
    "for idx, image_file in enumerate(art_examples):\n",
    "    plt.subplot(2, 3, idx+1)\n",
    "    painting = Image.open(os.path.join(art_folder, image_file))\n",
    "    plt.imshow(painting)\n",
    "    plt.title('Artistic Painting')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Comparison: Artistic Paintings vs Real-world Scenes', y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_metrics(image_file):\n",
    "    image = np.array(Image.open(image_file))\n",
    "    return np.mean(image)\n",
    "\n",
    "# Select random images for analysis\n",
    "sample_size = 50\n",
    "art_selection = random.sample(os.listdir(art_directory), sample_size)\n",
    "photo_selection = random.sample(os.listdir(photo_directory), sample_size)\n",
    "\n",
    "# Compute image metrics\n",
    "art_metrics = [calculate_image_metrics(os.path.join(art_directory, img)) for img in art_selection]\n",
    "photo_metrics = [calculate_image_metrics(os.path.join(photo_directory, img)) for img in photo_selection]\n",
    "\n",
    "# Display metric distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(photo_metrics, alpha=0.5, label='Photographs', bins=20, color='green')\n",
    "plt.hist(art_metrics, alpha=0.5, label='Artworks', bins=20, color='purple')\n",
    "plt.xlabel('Mean Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Mean Pixel Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Output summary statistics\n",
    "print(\"\\nImage Metric Summary:\")\n",
    "print(f\"Artworks - Average: {np.mean(art_metrics):.2f}, Standard Deviation: {np.std(art_metrics):.2f}\")\n",
    "print(f\"Photographs - Average: {np.mean(photo_metrics):.2f}, Standard Deviation: {np.std(photo_metrics):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.models import Model\n",
    "from keras.src import layers\n",
    "\n",
    "# Critic network\n",
    "def construct_critic():\n",
    "    input_layer = layers.Input(shape=(256, 256, 3))\n",
    "\n",
    "    h = layers.Conv2D(64, 3, strides=2, padding='same')(input_layer)\n",
    "    h = layers.LeakyReLU(0.2)(h)\n",
    "\n",
    "    h = layers.Conv2D(128, 3, strides=2, padding='same')(h)\n",
    "    h = layers.LeakyReLU(0.2)(h)\n",
    "\n",
    "    h = layers.Flatten()(h)\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=output_layer, name='critic')\n",
    "\n",
    "# Artist network (our primary model)\n",
    "def construct_artist():\n",
    "    input_layer = layers.Input(shape=(256, 256, 3))\n",
    "\n",
    "    # Encoding phase\n",
    "    h = layers.Conv2D(64, 3, padding='same')(input_layer)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.ReLU()(h)\n",
    "\n",
    "    h = layers.Conv2D(128, 3, padding='same')(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.ReLU()(h)\n",
    "\n",
    "    # Decoding phase\n",
    "    h = layers.Conv2DTranspose(64, 3, padding='same')(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.ReLU()(h)\n",
    "\n",
    "    # Final touch\n",
    "    output_layer = layers.Conv2D(3, 3, padding='same', activation='tanh')(h)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=output_layer, name='artist')\n",
    "\n",
    "# Instantiate the networks\n",
    "artist = construct_artist()\n",
    "critic = construct_critic()\n",
    "\n",
    "# Display network architectures\n",
    "artist.summary()\n",
    "critic.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.losses import BinaryCrossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "entropy_loss = BinaryCrossentropy()\n",
    "\n",
    "def artist_objective(synthetic_results):\n",
    "    return entropy_loss(tf.ones_like(synthetic_results), synthetic_results)\n",
    "\n",
    "def critic_objective(authentic_results, synthetic_results):\n",
    "    authentic_error = entropy_loss(tf.ones_like(authentic_results), authentic_results)\n",
    "    synthetic_error = entropy_loss(tf.zeros_like(synthetic_results), synthetic_results)\n",
    "    return authentic_error + synthetic_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.optimizers import Adam\n",
    "\n",
    "# Model hyperparameters\n",
    "SAMPLE_COUNT = 32\n",
    "TRAINING_CYCLES = 5\n",
    "\n",
    "# Optimization algorithms\n",
    "artist_optimizer = Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "critic_optimizer = Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Process and normalize a single image\n",
    "def prepare_image(img_file):\n",
    "    picture = Image.open(img_file)\n",
    "    # Transform to numpy array and scale to range [-1, 1]\n",
    "    picture = np.array(picture) / 127.5 - 1\n",
    "    return picture\n",
    "\n",
    "# Generate image batches from a specified folder\n",
    "def batch_generator(folder_path, batch_size=SAMPLE_COUNT):\n",
    "    image_list = os.listdir(folder_path)\n",
    "    while True:\n",
    "        # Randomize order at the beginning of each epoch\n",
    "        np.random.shuffle(image_list)\n",
    "        for i in range(0, len(image_list), batch_size):\n",
    "            current_batch = image_list[i:i + batch_size]\n",
    "            processed_images = []\n",
    "            for img_name in current_batch:\n",
    "                img_location = os.path.join(folder_path, img_name)\n",
    "                processed = prepare_image(img_location)\n",
    "                processed_images.append(processed)\n",
    "            yield np.array(processed_images)\n",
    "\n",
    "# Initialize batch generators\n",
    "art_batch_gen = batch_generator(art_folder)\n",
    "photo_batch_gen = batch_generator(photo_folder)\n",
    "\n",
    "# Determine iterations per epoch\n",
    "art_iterations = len(os.listdir(art_folder)) // SAMPLE_COUNT\n",
    "photo_iterations = len(os.listdir(photo_folder)) // SAMPLE_COUNT\n",
    "\n",
    "print(\"Data processing setup:\")\n",
    "print(f\"Artistic images iterations per epoch: {art_iterations}\")\n",
    "print(f\"Photographic images iterations per epoch: {photo_iterations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_photos, real_monet):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # generate fake Monet images\n",
    "        generated_monet = generator(real_photos, training=True)\n",
    "\n",
    "        # get discriminator decisions\n",
    "        real_output = discriminator(real_monet, training=True)\n",
    "        fake_output = discriminator(generated_monet, training=True)\n",
    "\n",
    "        # calculate losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # calculate gradients and update weights\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    # use tqdm for progress bar\n",
    "    for step in tqdm(range(min(monet_steps, photo_steps))):\n",
    "        real_photos = next(photo_generator)\n",
    "        real_monet = next(monet_generator)\n",
    "\n",
    "        gen_loss, disc_loss = train_step(real_photos, real_monet)\n",
    "\n",
    "        gen_losses.append(gen_loss)\n",
    "        disc_losses.append(disc_loss)\n",
    "\n",
    "    # print epoch results\n",
    "    avg_gen_loss = np.mean(gen_losses)\n",
    "    avg_disc_loss = np.mean(disc_losses)\n",
    "    print(f\"Generator Loss: {avg_gen_loss:.4f}\")\n",
    "    print(f\"Discriminator Loss: {avg_disc_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record performance metrics for visualization\n",
    "cycles = range(1, 6)\n",
    "artist_performance = [1.6934, 1.7110, 1.6815, 1.4591, 1.0369]\n",
    "critic_performance = [1.1160, 1.0479, 0.9797, 0.9489, 1.0955]\n",
    "\n",
    "# Visualize performance trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cycles, critic_performance, 'g', label='Critic Network Loss')\n",
    "plt.plot(cycles, artist_performance, 'p', label='Artist Network Loss')\n",
    "plt.title('Performance Metrics: Artist vs Critic Networks')\n",
    "plt.xlabel('Training Cycle')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Outcome Summary:\")\n",
    "print(f\"Final Artist Network Loss: {artist_performance[-1]:.4f}\")\n",
    "print(f\"Final Critic Network Loss: {critic_performance[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a set of test images\n",
    "evaluation_set = next(photo_batch_gen)\n",
    "\n",
    "# Create artistic renditions\n",
    "artistic_renditions = artist(evaluation_set, training=False)\n",
    "\n",
    "# Function to rescale normalized images for display\n",
    "def prepare_for_display(image):\n",
    "    image = (image + 1) * 127.5\n",
    "    return np.clip(image, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Visualize original and artistic versions\n",
    "plt.figure(figsize=(15, 8))\n",
    "for idx in range(3):  # display 3 examples\n",
    "    # Source photograph\n",
    "    plt.subplot(2, 3, idx + 1)\n",
    "    plt.imshow(prepare_for_display(evaluation_set[idx]))\n",
    "    plt.title('Source Photograph')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Artistic rendition\n",
    "    plt.subplot(2, 3, idx + 4)\n",
    "    plt.imshow(prepare_for_display(artistic_renditions[idx]))\n",
    "    plt.title('Artistic Rendition')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Comparison: Source Photographs vs Artistic Renditions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Establish a directory for our artistic creations\n",
    "if not os.path.exists('artistic_creations'):\n",
    "    os.makedirs('artistic_creations')\n",
    "\n",
    "# Function to produce and store artistic renditions\n",
    "def create_artistic_portfolio(artist_model, portfolio_size=7500):\n",
    "    print(f\"Crafting a portfolio of {portfolio_size} artistic pieces...\")\n",
    "\n",
    "    # Prepare batches from the photo collection\n",
    "    photo_collection = batch_generator(photo_folder, SAMPLE_COUNT)\n",
    "\n",
    "    for i in tqdm(range(0, portfolio_size, SAMPLE_COUNT)):\n",
    "        # Obtain a set of photographs\n",
    "        photo_set = next(photo_collection)\n",
    "\n",
    "        # Transform photos into artistic renditions\n",
    "        artistic_pieces = artist_model(photo_set, training=False)\n",
    "\n",
    "        # Preserve each piece in the set\n",
    "        for j, piece in enumerate(artistic_pieces):\n",
    "            if i + j < portfolio_size:\n",
    "                # Convert to displayable format\n",
    "                piece_array = prepare_for_display(piece.numpy())\n",
    "                art_piece = Image.fromarray(piece_array)\n",
    "                art_piece.save(f'artistic_creations/artwork_{i+j}.jpg', 'JPEG')\n",
    "\n",
    "# Generate and store the artistic portfolio\n",
    "create_artistic_portfolio(artist)\n",
    "\n",
    "# Compile the portfolio into a zip archive\n",
    "print(\"Assembling the portfolio for submission...\")\n",
    "with zipfile.ZipFile('artistic_portfolio.zip', 'w') as portfolio_archive:\n",
    "    for artwork in os.listdir('artistic_creations'):\n",
    "        portfolio_archive.write(os.path.join('artistic_creations', artwork),\n",
    "                arcname=artwork)\n",
    "\n",
    "print(\"Portfolio compilation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
